@techreport{Quiring19modularity,
   author =          {Quiring, Benjamin G. and Vassilevski, Panayot S.},
   title =           {Properties of the Graph Modularity Matrix and its Applications},
   number =          {LLNL-TR-779424},
   institution =     {Lawrence Livermore National Lab},
   address =         {Livermore, California},
   abstractNote =    {We study the popular modularity matrix and respective functional used in
                      connection with graph clustering and derive some properties useful when
                      performing vertex aggregation of the associated graph. These properties are
                      employed in the derivation of a multilevel parallel pairwise aggregation
                      algorithm. Some illustrative examples which include algebraic multigrid
                      (AMG) coarsening that follows strong direction of anisotropy in finite
                      element problems as well as comparative performance results of the studied
                      algorithm are presented.},
   keywords =        {graphs, multilevel algorithms, graph modularity},
   doi =             {10.2172/1529818},
   journal =         {},
   number =          {},
   volume =          {},
   place =           {United States},
   year =            {2019},
   month =           {6},
   author1_url =     {},
   author1_email =   {},
   author2_url =     {},
   author2_email =   {},
   pages =           {13},
   file =            {}
}

@techreport{Quiring19embed,
  title =           {Multilevel Graph Embedding},
  author =          {Quiring, BG and Vassilevski, PS},
  year =            {2019},
  number =          {LLNL-TR-779424},
  institution =     {Lawrence Livermore National Lab},
  address =         {Livermore, California},
}

@misc{BoL,
   title =           {Food Services and Drinking Places: NAICS 722},
   howpublished =    {\url{https://www.bls.gov/iag/tgs/iag722.htm}},
   note =            {Accessed: 2020-04-06},
}

@misc{mtlynch,
   title =           {Ingredient Phrase Tagger},
   author =          {Lynch, Michael},
   howpublished =    {\url{https://github.com/aujxn/ingredient-phrase-tagger}},
   note =            {Accessed: 2020-02-22},
}

@inproceedings{Majumder19,
    title =          {Generating Personalized Recipes from Historical User Preferences},
    author =         {Majumder, Bodhisattwa Prasad and Li, Shuyang and Ni, Jianmo and McAuley,
                      Julian},
    booktitle =      {Proceedings of the 2019 Conference on Empirical Methods in Natural Language
                      Processing and the 9th International Joint Conference on Natural Language
                      Processing (EMNLP-IJCNLP)},
    month =          {nov},
    year =           {2019},
    address =        {Hong Kong, China},
    publisher =      {Association for Computational Linguistics},
    url =            {https://www.aclweb.org/anthology/D19-1613},
    doi =            {10.18653/v1/D19-1613},
    pages =          {5976--5982},
    abstract =       {Existing approaches to recipe generation are unable to create recipes for
                      users with culinary preferences but incomplete knowledge of ingredients in
                      specific dishes. We propose a new task of personalized recipe generation to
                      help these users: expanding a name and incomplete ingredient details into
                      complete natural-text instructions aligned with the user{'}s historical
                      preferences. We attend on technique- and recipe-level representations of a
                      user{'}s previously consumed recipes, fusing these {`}user-aware{'}
                      representations in an attention fusion layer to control recipe text
                      generation. Experiments on a new dataset of 180K recipes and 700K
                      interactions show our model{'}s ability to generate plausible and
                      personalized recipes compared to non-personalized baselines.},
}

@article{Makhzani16,
	title = {Adversarial {Autoencoders}},
	url = {http://arxiv.org/abs/1511.05644},
	abstract = {In this paper, we propose the “adversarial autoencoder” (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classiﬁcation, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classiﬁcation tasks.},
	language = {en},
	urldate = {2020-04-06},
	journal = {arXiv:1511.05644 [cs]},
	author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
	month = {May},
	year = {2016},
	note = {arXiv: 1511.05644},
	keywords = {Computer Science - Machine Learning},
	file = {Makhzani et al. - 2016 - Adversarial Autoencoders.pdf:/home/austen/Zotero/storage/8C99Y9ER/Makhzani et al. - 2016 - Adversarial Autoencoders.pdf:application/pdf}
}

@inproceedings{Goodfellow14,
	title = {Generative {Adversarial} {Nets}},
	language = {en},
   booktitle = {Proceedings of the 2014 Conference on Advances in Neeural Information Processing Systems (NIPS)},
   pages = {2672-2680},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
   year = {2014},
   month = {June},
	pages = {9},
	file = {Goodfellow et al. - Generative Adversarial Nets.pdf:/home/austen/Zotero/storage/4V8H6R8D/Goodfellow et al. - Generative Adversarial Nets.pdf:application/pdf}
}

@inproceedings{Radford16,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	language = {en},
	urldate = {2020-04-06},
	journal = {arXiv:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
   booktitle = {Proceedings of the 2016 International Conference on Learning Representations (ICLR)},
	month = jan,
	year = {2016},
	note = {arXiv: 1511.06434},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Under review as a conference paper at ICLR 2016},
	file = {Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:/home/austen/Zotero/storage/E34QYLX2/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf}
}

@inproceedings{Lafferty01,
	title = {Conditional {Random} {Fields}: {Probabilistic} {Models} for {Segmenting} and {Labeling} {Sequence} {Data}},
   booktitle = {Proceedings of the 18th International Conference on Machine Learning 2001 (ICML 2001)},
   month = {jun},
   year = {2001},
   pages = {282--289},
   url = {https://dl.acm.org/doi/10.5555/645530.655813},
	abstract = {We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.},
	language = {en},
	author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando C N},
	pages = {10},
	file = {Lafferty et al. - Conditional Random Fields Probabilistic Models fo.pdf:/home/austen/Zotero/storage/JS8L6VBY/Lafferty et al. - Conditional Random Fields Probabilistic Models fo.pdf:application/pdf}
}

@book{Algorithms99,
   author = {Thomas H. Cormen and Charles E. Leiserson and Ronald L. Rivest},
   title = {Introduction to Algorithms},
   publisher = {MIT Press},
   year = {1999},
   isbn = {9780262033848}
}

@book{Networks10,
   author = {Mark Newman},
   title = {Networks: An Introduction},
   publisher = {Oxford University Press},
   year = {2010},
   isbn = {9780199206650},
}

@article{Blondel08,
	title = {Fast unfolding of communities in large networks},
	volume = {2008},
	issn = {1742-5468},
	url = {http://arxiv.org/abs/0803.0476},
	doi = {10.1088/1742-5468/2008/10/P10008},
	abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection method in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown ﬁrst by identifying language communities in a Belgian mobile phone network of 2.6 million customers and by analyzing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also veriﬁed on ad-hoc modular networks.},
	language = {en},
	number = {10},
	urldate = {2020-04-18},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Blondel, Vincent D. and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
	month = oct,
	year = {2008},
	note = {arXiv: 0803.0476},
	keywords = {Computer Science - Computers and Society, Computer Science - Data Structures and Algorithms, Condensed Matter - Statistical Mechanics, Physics - Physics and Society},
	pages = {P10008},
	annote = {Comment: 6 pages, 5 figures, 1 table; new version with new figures in order to clarify our method, where we look more carefully at the role played by the ordering of the nodes and where we compare our method with that of Wakita and Tsurumi},
	file = {Blondel et al. - 2008 - Fast unfolding of communities in large networks.pdf:/home/austen/Zotero/storage/QL3BTECP/Blondel et al. - 2008 - Fast unfolding of communities in large networks.pdf:application/pdf}
}

@article{Jones91,
	title = {A {Parallel} {Graph} {Coloring} {Heuristic}},
	volume = {14},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/0914041},
	doi = {10.1137/0914041},
	abstract = {The problem of computing good graph colorings arises in many diverse applications, such as in the estimation of sparse Jacobians and in the development of e cient, parallel iterative methods for solving sparse linear systems. In this paper we present an asynchronous graph coloring heuristic well suited to distributed memory parallel computers. We present experimental results obtained on an Intel iPSC/860 which demonstrate that, for graphs arising from nite element applications, the heuristic exhibits scalable performance and generates colorings usually within three or four colors of the best-known linear time sequential heuristics. For bounded degree graphs, we show that the expected running time of the heuristic under the P-RAM computation model is bounded by EO(log(n)= log log(n)). This bound is an improvement over the previously known best upper bound for the expected running time of a random heuristic for the graph coloring problem.},
	language = {en},
	number = {3},
	urldate = {2020-04-18},
	journal = {SIAM Journal on Scientific Computing},
	author = {Jones, Mark T. and Plassmann, Paul E.},
	month = may,
	year = {1993},
	pages = {654--669},
	file = {Jones and Plassmann - 1993 - A Parallel Graph Coloring Heuristic.pdf:/home/austen/Zotero/storage/VI5U6QP3/Jones and Plassmann - 1993 - A Parallel Graph Coloring Heuristic.pdf:application/pdf}
}
 
@article{Karypis98,
	title = {A {Fast} and {High} {Quality} {Multilevel} {Scheme} for {Partitioning} {Irregular} {Graphs}},
	volume = {20},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/S1064827595287997},
	doi = {10.1137/S1064827595287997},
	abstract = {Recently, a number of researchers have investigated a class of graph partitioning algorithms that reduce the size of the graph by collapsing vertices and edges, partition the smaller graph, and then uncoarsen it to construct a partition for the original graph [Bui and Jones, Proc. of the 6th SIAM Conference on Parallel Processing for Scientiﬁc Computing, 1993, 445–452; Hendrickson and Leland, A Multilevel Algorithm for Partitioning Graphs, Tech. report SAND 93-1301, Sandia National Laboratories, Albuquerque, NM, 1993]. From the early work it was clear that multilevel techniques held great promise; however, it was not known if they can be made to consistently produce high quality partitions for graphs arising in a wide range of application domains. We investigate the eﬀectiveness of many diﬀerent choices for all three phases: coarsening, partition of the coarsest graph, and reﬁnement. In particular, we present a new coarsening heuristic (called heavy-edge heuristic) for which the size of the partition of the coarse graph is within a small factor of the size of the ﬁnal partition obtained after multilevel reﬁnement. We also present a much faster variation of the Kernighan–Lin (KL) algorithm for reﬁning during uncoarsening. We test our scheme on a large number of graphs arising in various domains including ﬁnite element methods, linear programming, VLSI, and transportation. Our experiments show that our scheme produces partitions that are consistently better than those produced by spectral partitioning schemes in substantially smaller time. Also, when our scheme is used to compute ﬁll-reducing orderings for sparse matrices, it produces orderings that have substantially smaller ﬁll than the widely used multiple minimum degree algorithm.},
	language = {en},
	number = {1},
	urldate = {2020-04-18},
	journal = {SIAM Journal on Scientific Computing},
	author = {Karypis, George and Kumar, Vipin},
	month = jan,
	year = {1998},
	pages = {359--392},
	file = {Karypis and Kumar - 1998 - A Fast and High Quality Multilevel Scheme for Part.pdf:/home/austen/Zotero/storage/TJMHJPGN/Karypis and Kumar - 1998 - A Fast and High Quality Multilevel Scheme for Part.pdf:application/pdf}
}

@article{Luby86,
	title = {A {Simple} {Parallel} {Algorithm} for the {Maximal} {Independent} {Set} {Problem}},
	abstract = {Two basic design strategies are used to develop a very simple and fast parallel algorithms for the maximal independent set (MIS) problem. The first strategy consists of assigning identical copies of a simple algorithm to small local portions of the problem input. The algorithm is designed so that when the copies are executed in parallel the correct problem output is produced very quickly. A very simple Monte Carlo algorithm for the MIS problem is presented which is based upon this strategy. The second strategy is a general and powerful technique for removing randomization from algorithms. This strategy is used to convert the Monte Carlo algorithm for this MIS problem into a simple deterministic algorithm with the same parallel running time.},
	language = {en},
	author = {Luby, M},
	pages = {18},
	file = {Luby - A Simple Parallel Algorithm for the Maximal Indepe.pdf:/home/austen/Zotero/storage/4GT98B3T/Luby - A Simple Parallel Algorithm for the Maximal Indepe.pdf:application/pdf}
}

@inproceedings{Rossi15,
	title = {The {Network} {Data} {Repository} with {Interactive} {Graph} {Analytics} and {Visualization}},
   booktitle = {Proceedings of the 29th Conference on Artificial Intelligence (AAAI)},
	abstract = {NR) is the ﬁrst interactive data repository with a web-based platform for visual interactive analytics. Unlike other data repositories (e.g., UCI ML Data Repository, and SNAP), the network data repository (networkrepository.com) allows users to not only download, but to interactively analyze and visualize such data using our web-based interactive graph analytics platform. Users can in real-time analyze, visualize, compare, and explore data along many different dimensions. The aim of NR is to make it easy to discover key insights into the data extremely fast with little effort while also providing a medium for users to share data, visualizations, and insights. Other key factors that differentiate NR from the current data repositories is the number of graph datasets, their size, and variety. While other data repositories are static, they also lack a means for users to collaboratively discuss a particular dataset, corrections, or challenges with using the data for certain applications. In contrast, NR incorporates many social and collaborative aspects that facilitate scientiﬁc research, e.g., users can discuss each graph, post observations, and visualizations.},
	language = {en},
	author = {Rossi, Ryan A and Ahmed, Nesreen K},
	pages = {2},
   year = {2015},
	file = {Rossi and Ahmed - The Network Data Repository with Interactive Graph.pdf:/home/austen/Zotero/storage/FXMIQD2X/Rossi and Ahmed - The Network Data Repository with Interactive Graph.pdf:application/pdf}
}

@article{Ahn11,
  title={Flavor network and the principles of food pairing},
  author={Ahn, Yong-Yeol and Ahnert, Sebastian E and Bagrow, James P and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  journal={Scientific reports},
  volume={1},
  pages={196},
  year={2011},
  publisher={Nature Publishing Group}
}
@mastersthesis{bostan2017,
   author = {Laura Ana Maria Bostan},
   title = {Ingredient-driven Recipe Generation Using Neural and Distributional Models},
   school = {University of Trento},
   year = {2017},
   month = {July},
   annote = {In this paper Bostan investigates computational ability to generate
   recipe instructions as natural language from a set of ingredients.
   The primary focus is on connecting ingredients with semantic instructions and
   finding ways to connect those instructions into sensible event chains. These event
   chains are then translated into natural language using standard distrobutional 
   naturual language processing techniques such as word2vec and bag of words. Section
   one introduces the motivation and details the research question. The goal is to
   generate recipes utilizing data from aggregated recipes using unsupervised
   learning techniques. Section two gives a history of related work and details the
   inspiration for many of the techniques utilized. Section three describes the
   datasets used. Public datasets of recipes such as OpenRecipes and Now you're
   cooking provide about 200,000 recipes for processing. This section also details
   the general data-wrangling techniques and generalized statistics. Bostan also
   found cuisine can be identified from the ingredients of the recipe. Section four
   details the methods of the study. Botan created two co-occurrence matrices.
   One counting the pairs of co-occurring ingredients and the other counting pairs
   of ingredient - action (ie. stir, bake, whisk) occurrences. To create the
   ingredient vector space, Bostan reduced the dimension of the Ingredient matrix
   using singular value decomposition. This information is used to train an
   architecture or recurrent neural network called a Long-short-term memory network.
   To test the network, Bostan trained on 80\% of the data then tried to generate
   instructions for the remaining recipes. Bostan found promising results but left
   many components to future research.}
}
